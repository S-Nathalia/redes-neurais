{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"awdK9Wf4j7yw"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.layers import Embedding\n","from matplotlib import pyplot as plt\n","import random\n","\n","from time import time\n","id_exec = int(time())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V28F0cPyrEOx"},"outputs":[],"source":["def clean_book(book, all_books):\n","    with open (book) as f:\n","        for line in f.readlines():\n","            line = line.lower().strip()\n","            line = line + ' '\n","            if 'page' in line and 'rowling' in line:\n","                continue\n","            all_books += line\n","    for c in ['”', '\\\"', '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '—', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']:\n","        all_books.replace(c, \"\")\n","    return all_books"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lwB-Be4oHfm"},"outputs":[],"source":["from google.colab import files\n","\n","def plot_metric(losses_train, losses_val, id_exec):\n","    fig = plt.figure()\n","    plt.plot([x for x in range(epochs)], losses_train)\n","    plt.plot([x for x in range(epochs)], losses_val)\n","    plt.title('LSTM Model Loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'])\n","    fig.savefig(str(id_exec)+'.png')\n","    files.download(str(id_exec)+'.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VzgYdkHkCt8"},"outputs":[],"source":["book = ''\n","pross_book = \"/content/drive/MyDrive/Book3.txt\"\n","book = clean_book(pross_book, book)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpK4IHwSiano"},"outputs":[],"source":["tokenizer = Tokenizer(oov_token=\"<OOV>\", filters='!\"#$%&()*+,-—./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n","tokenizer.fit_on_texts([book])\n","\n","vocab = tokenizer.word_index\n","encoded = tokenizer.texts_to_sequences([book])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mksEavRsfYSi"},"outputs":[],"source":["context = 5\n","vocab_size = 1000\n","\n","sequences = []\n","for i in range(len(encoded[0])-context+1):\n","    sequences.append(encoded[0][i:i+context])\n","sequences = sequences[:vocab_size]\n","np.random.shuffle(sequences)"]},{"cell_type":"markdown","source":["## Onehot para RNN simples"],"metadata":{"id":"2lIbNJ3ESBHR"}},{"cell_type":"code","source":["# # Aplica o onehot em toda a base\n","# sequences = list(map(\n","#     lambda x: [np.array((tf.transpose(x2), vocab_size)) for x2 in x],\n","#     sequences))"],"metadata":{"id":"7eLbAw3dR_7X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Separação da base de dados"],"metadata":{"id":"wogqRk6HSLvO"}},{"cell_type":"code","source":["porcent = [0.7, 0.2, 0.10]\n","tam = len(sequences)\n","sequences_train = sequences[:int(tam*porcent[0])]\n","sequences_val = sequences[int(tam*porcent[0]):int(tam*(porcent[0]+porcent[1]))]\n","sequences_test =  sequences[int(tam*(porcent[0]+porcent[1])):]\n","# print(f\"Len Datasets: [{len(sequences_train)}, {len(sequences_val)}, {len(sequences_test)}]\")"],"metadata":{"id":"SBSKQ8ExSJZU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baixando embbeding para rede"],"metadata":{"id":"L83w7DwJyLVc"}},{"cell_type":"code","source":["!wget --no-check-certificate \\\n","     http://nlp.stanford.edu/data/glove.6B.zip \\\n","     -O /tmp/glove.6B.zip\n","\n","import os\n","import zipfile\n","with zipfile.ZipFile('/tmp/glove.6B.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/tmp/glove')"],"metadata":{"id":"LlXgszxEsPpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings_index = {}\n","f = open('/tmp/glove/glove.6B.100d.txt')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.array(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zCMz-cvYs8ot","executionInfo":{"status":"ok","timestamp":1650593784089,"user_tz":180,"elapsed":19185,"user":{"displayName":"Nathalia Santos","userId":"18223315061685652104"}},"outputId":"8eb379fa-570e-44fc-dada-9d7a9c7f3b98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 400000 word vectors.\n"]}]},{"cell_type":"code","source":["embedding_matrix = np.zeros((len(vocab) + 1, 100))\n","for word, i in vocab.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector"],"metadata":{"id":"DP31378EtXHG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Aplicando embedding na base de dados"],"metadata":{"id":"bE0D4RryybVE"}},{"cell_type":"code","source":["# Aplica Embedding na base de dados\n","embedding_layer = Embedding(\n","    input_dim=len(vocab) + 1, output_dim=100,\n","    weights=[embedding_matrix], input_length=1, trainable=False)\n","\n","aux = list(map(lambda x: [embedding_layer(c) for c in x], sequences_train))\n","for c_i in range(len(aux)): sequences_train[c_i] = aux[c_i][:-1] + [sequences_train[c_i][-1]]\n","random.shuffle(sequences_train)\n","aux = list(map(lambda x: [embedding_layer(c) for c in x], sequences_test))\n","for c_i in range(len(aux)): sequences_test[c_i] = aux[c_i][:-1] + [sequences_test[c_i][-1]]\n","random.shuffle(sequences_test)\n","aux = list(map(lambda x: [embedding_layer(c) for c in x], sequences_val))\n","for c_i in range(len(aux)): sequences_val[c_i] = aux[c_i][:-1] + [sequences_val[c_i][-1]]\n","random.shuffle(sequences_val)"],"metadata":{"id":"s06MOR6lSwh0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RNN simples com embedding \n","### PS: após os testes com embedding a rede rnn simples foi modificada para funcionar apenas com essa configuração de dados"],"metadata":{"id":"cd9830DYyfkV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"huZpelGs2vd6"},"outputs":[],"source":["class EmbeddingModel(tf.keras.models.Model):\n","    def __init__(self, num_hiddens, vocab_size):\n","        super(EmbeddingModel, self).__init__()\n","        # célular simples de rnn (retornando state)\n","        self.rnn_cell = tf.keras.layers.SimpleRNNCell(\n","            num_hiddens, kernel_initializer='glorot_uniform')\n","        self.rnn = tf.keras.layers.RNN(\n","            self.rnn_cell, time_major=False,\n","            return_sequences=False, return_state=True)\n","\n","        self.dense_1 = tf.keras.layers.Dense(vocab_size, activation='relu')\n","        self.dense_2 = tf.keras.layers.Dense(vocab_size, activation='softmax')\n","\n","    def call(self, inputs, state, training=True):\n","        output, state = self.rnn(inputs, state)\n","        output = self.dense_1(tf.reshape(output, (-1, output.shape[-1])))\n","        output = self.dense_2(tf.reshape(output, (-1, output.shape[-1])))\n","        return output, state\n","\n","    # Utilizado na chamada da primeira iteração da recorrência\n","    def get_begin_state(self, batch_size):\n","      return self.rnn_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)"]},{"cell_type":"markdown","source":["# RNN profunda com embedding"],"metadata":{"id":"hZ0UIJqnyu3N"}},{"cell_type":"code","source":["class DeepEmbeddingModel(tf.keras.models.Model):\n","    def __init__(self, num_hiddens, vocab_size):\n","        super(DeepEmbeddingModel, self).__init__()\n","        # célular simples de rnn (retornando state)\n","        self.rnn_cell = tf.keras.layers.SimpleRNNCell(\n","            num_hiddens, kernel_initializer='glorot_uniform')\n","        self.rnn = tf.keras.layers.RNN(\n","            self.rnn_cell, time_major=False,\n","            return_sequences=False, return_state=True)\n","\n","        self.dense_1 = tf.keras.layers.Dense(vocab_size, activation='tanh')\n","        self.dense_2 = tf.keras.layers.Dense(vocab_size, activation='tanh')\n","        self.dense_3 = tf.keras.layers.Dense(vocab_size, activation='tanh')\n","        self.dense_4 = tf.keras.layers.Dense(vocab_size, activation='tanh')\n","\n","    def call(self, inputs, state, training=True):\n","        output, state = self.rnn(inputs, state)\n","        output = self.dense_1(tf.reshape(output, (-1, output.shape[-1])))\n","        output = self.dense_2(tf.reshape(output, (-1, output.shape[-1])))\n","        output = self.dense_3(tf.reshape(output, (-1, output.shape[-1])))\n","        output = self.dense_4(tf.reshape(output, (-1, output.shape[-1])))\n","        return output, state\n","\n","    # Utilizado na chamada da primeira iteração da recorrência\n","    def get_begin_state(self, batch_size):\n","      return self.rnn_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)"],"metadata":{"id":"CMSgdxgcxocO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Função para predizer a próxima palavra"],"metadata":{"id":"C4TsZ3gg9Czr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rG9vCPLVPHkX"},"outputs":[],"source":["def predict(prefix, num_preds, model, vocab, tokenizer):\n","    prefix = tokenizer.texts_to_sequences(prefix.split())\n","    prediction = prefix.copy()\n","\n","    aux = [embedding_layer(np.array(x)) for x in prefix]\n","\n","    state = model.get_begin_state(1)\n","    last_word = aux[-1]\n","\n","    aux = np.array(aux, dtype=np.float32)\n","    last_word = np.array([last_word], dtype=np.float32)\n","\n","    for p in aux:\n","        print(len(model(np.array([p]), state)))\n","        _, state = model(np.array([p]), state)\n","\n","    for i in range(num_preds):\n","        pred, state = model(last_word, state)\n","        pred = np.argmax(pred, axis=1)\n","        last_word = np.array([embedding_layer(pred)])\n","        prediction.append(pred)\n","\n","    prediction = list(map(lambda x: x[0], prediction)) # Tira da lista de list\n","    return tokenizer.sequences_to_texts(np.array([prediction]))"]},{"cell_type":"markdown","source":["# função para treinar os modelos"],"metadata":{"id":"p715IGBr9PUC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3RgmEuhZMxS"},"outputs":[],"source":["def fit(model, sequences_train, sequences_val, epochs, losses_train, losses_val):\n","    for epoch in range(epochs):\n","        sum_loss_train = 0.0\n","\n","        for i, sample in enumerate(sequences_train):\n","            # Separa samples e y\n","            x = np.array([sample[0:-1]], dtype=np.float32);\n","            # y = np.array([sample[-1]], dtype=np.float32)\n","            y_onehot = np.asarray([tf.one_hot(tf.transpose(sample[-1]), vocab_size)]) # gambiarra?????????\n","\n","            state = model.get_begin_state(1) # State padrão\n","            y_pred = None # Predição final\n","\n","          # salva cada passo para cálculo do gradiente\n","            with tf.GradientTape() as tape:\n","                for x_sample in x:\n","                    y_pred, state = model(np.array([x_sample]), state, training=True)\n","                loss_value = loss_func(y_onehot, y_pred)\n","\n","            grads = tape.gradient(loss_value, model.trainable_weights)\n","            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","            sum_loss_train += loss_value.numpy()\n","\n","      #_________________________________________________________________________\n","            # if i % 50 == 0:\n","            #     print(f\"[exec train: {i}/{len(sequences_train)}] [epo: {epoch}/{epochs-1}] Loss: {sum_loss_train/(i+1)}\", end=\"\\r\")\n","        print(f\"Final Mean Train Loss: {sum_loss_train/len(sequences_train):.8f}\")\n","        losses_train.append(sum_loss_train/len(sequences_train))\n","        print(\"\")\n","        #_________________________________________________________________________\n","\n","      # validação dos dados\n","        sum_loss_val = 0.0\n","        for i, sample in enumerate(sequences_val):\n","            # Separa samples de y\n","            x = np.array([sample[0:-1]], dtype=np.float32);\n","            # y = np.array(sample[-1], dtype=np.float32)\n","            y_onehot = np.asarray([tf.one_hot(tf.transpose(sample[-1]), vocab_size)])\n","\n","\n","            state = model.get_begin_state(1) # State padrão\n","            y_pred = None # Predição final\n","\n","            # salva cada passo para cálculo do gradiente\n","            for x_sample in x:\n","                y_pred, state = model(np.array([x_sample]), state, training=False)\n","            loss_value = loss_func(y_onehot, y_pred)\n","\n","            sum_loss_val += loss_value.numpy()\n","\n","        #_________________________________________________________________________\n","            # if i % 50 == 0:\n","            #     print(f\"[exec val  : {i}/{len(sequences_val)}] [epo: {epoch}/{epochs-1}] Loss: {sum_loss_val/(i+1)}\", end=\"\\r\")\n","        print(f\"Final Mean Val Loss  : {sum_loss_val/len(sequences_val):.8f}\")\n","        losses_val.append(sum_loss_val/len(sequences_val))\n","        print(\"\")\n","        # print() # Limitador de print a cada época\n","     \n","        inp = 'harry potter was a'\n","        frase = predict(inp, 5, model, vocab, tokenizer)    \n","        print(f\"{frase[0]}\")\n","\n","    return losses_train, losses_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVfbW6xnTcyG"},"outputs":[],"source":["print(f\"Iniciando execução: [{id_exec}]\")\n","\n","losses_train, losses_val = [], []\n","num_hiddens, batch_size, lr, epochs = 500, 1, 0.5, 10\n","optimizer = tf.keras.optimizers.Adam(lr)\n","loss_func = tf.keras.losses.CategoricalCrossentropy() \n","\n","model = EmbeddingModel(num_hiddens, vocab_size)\n","losses_train, losses_val = fit(model, sequences_train, sequences_val, epochs, losses_train, losses_val)\n","\n","plot_metric(losses_train, losses_val, id_exec)"]},{"cell_type":"code","source":["# model.save_weights('Lista2_Q1_RNN_Nathalia_Santos')"],"metadata":{"id":"dTS1rZIFecri"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Campo para digitar frase de predição"],"metadata":{"id":"Bx2O4lXl-V1N"}},{"cell_type":"code","source":["inp = input(\"Digite sua frase: \")\n","frase = predict(inp, 5, model, vocab, tokenizer)\n","print(f\"{frase[0]}\")"],"metadata":{"id":"T6k1MbvAZrDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Iniciando execução: [{id_exec}]\")\n","\n","losses_train2, losses_val2 = [], []\n","num_hiddens, batch_size, lr, epochs = 500, 1, 0.005, 4\n","optimizer = tf.keras.optimizers.Adam(lr)\n","loss_func = tf.keras.losses.CategoricalCrossentropy() \n","\n","model_deep = DeepEmbeddingModel(num_hiddens, vocab_size)\n","losses_train2, losses_val2 = fit(model_deep, sequences_train, sequences_val, epochs, losses_train2, losses_val2)\n","\n","# plot_metric(losses_train2, losses_val2, id_exec)"],"metadata":{"id":"-yITjigGYUjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model_deep.save_weights('Lista2_Q1_RNN_profunda_Nathalia_Santos')"],"metadata":{"id":"QvFF3q6MquKC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Campo para digitar frase de predição"],"metadata":{"id":"6IMqQb6z-QCM"}},{"cell_type":"code","source":["inp = input(\"Digite sua frase: \")\n","frase = predict(inp, 5, model_deep, vocab, tokenizer)\n","print(f\"{frase[0]}\")"],"metadata":{"id":"kj5mQV-9pWUC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LSTM"],"metadata":{"id":"tHnZpFBy-NN7"}},{"cell_type":"code","source":["class LSTMModel(tf.keras.models.Model):\n","    def __init__(self, num_hiddens, vocab_size):\n","        super(LSTMModel, self).__init__()\n","        # célular simples de rnn (retornando state)\n","        self.lstm_cell = tf.keras.layers.LSTMCell(\n","            num_hiddens, kernel_initializer='glorot_uniform',\n","            recurrent_initializer='orthogonal')\n","        self.lstm = tf.keras.layers.RNN(\n","            self.lstm_cell, time_major=False,\n","            return_sequences=False, return_state=True)\n","\n","        self.dense_1 = tf.keras.layers.Dense(vocab_size, activation='tanh')\n","        self.dense_2 = tf.keras.layers.Dense(vocab_size, activation='tanh')\n","        self.dense_3 = tf.keras.layers.Dense(vocab_size, activation='tanh')\n","        self.dense_4 = tf.keras.layers.Dense(vocab_size, activation='softmax')\n","        # self.vocab_size = vocab_size\n","\n","    def call(self, inputs, state, training=True):\n","    \n","        output, state, _ = self.lstm(inputs, state)\n","        output = self.dense_1(tf.reshape(output, (-1, output.shape[-1])))\n","        output = self.dense_2(tf.reshape(output, (-1, output.shape[-1])))\n","        output = self.dense_3(tf.reshape(output, (-1, output.shape[-1])))\n","        output = self.dense_4(tf.reshape(output, (-1, output.shape[-1])))\n","        return output, state\n","\n","    # Utilizado na chamada da primeira iteração da recorrência\n","    def get_begin_state(self, batch_size):\n","      return self.lstm_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)"],"metadata":{"id":"QYSVMJwBpjoY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Iniciando execução: [{id_exec}]\")\n","\n","losses_train3, losses_val3 = [], []\n","num_hiddens, batch_size, lr, epochs = 500, 1, 0.0005, 5\n","optimizer = tf.keras.optimizers.Adam(lr)\n","loss_func = tf.keras.losses.CategoricalCrossentropy() \n","\n","lstm = LSTMModel(num_hiddens, vocab_size)\n","losses_train3, losses_val3 = fit(lstm, sequences_train, sequences_val, epochs,losses_train3, losses_val3)\n","\n","# plot_metric(losses_train3, losses_val3, id_exec)"],"metadata":{"id":"CfjezWy8FzKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lstm.save_weights('Lista2_Q1_LSTM_profundo_Nathalia_Santos')"],"metadata":{"id":"4Utx_QP3sf9R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Campo para digitar frase de predição"],"metadata":{"id":"Gh5T29Zx-eAN"}},{"cell_type":"code","source":["inp = input(\"Digite sua frase: \")\n","frase = predict(inp, 5, lstm, vocab, tokenizer)\n","print(f\"{frase[0]}\")"],"metadata":{"id":"fLdlkl859c8J"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Lista 2.ipynb","provenance":[],"mount_file_id":"1d0QYleolMtjVfBlTfXQKr1pcahMSof1b","authorship_tag":"ABX9TyNJtX8w7fcPOTaz4OUqg4Et"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}